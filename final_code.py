# -*- coding: utf-8 -*-
"""Final_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvFUlgoTlNEjYxDeQGaZ4aJiTocbqoyV
"""

!pip freeze > requirements.txt

!pip install meteostat

#Necessary imports
import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Input, LSTM, GRU, Dropout
from keras.optimizers import SGD
from sklearn.preprocessing import MinMaxScaler

#Weather API import only and plots
from datetime import datetime
import matplotlib.pyplot as plt
from meteostat import Point, Hourly

#input CSV file
df=pd.read_csv('/content/SmartWatts_Interval_Meter_Usage.csv') #SmartWatts
df

df.describe()

# Fetches Weather Data from meteostat
def fetch_weather_data(codeName, start_year, start_month, start_day, end_year, end_month, end_day):
  # Define the time period and location
  start = datetime(start_year, start_month, start_day) #changed dates need to revert back to original start and end time
  end = datetime(end_year, end_month, end_day)
  Houston = Point(29.7604, -95.3698, 13)

  # Fetch hourly weather data
  df1 = Hourly(Houston, start, end)
  weather_df = df1.fetch()

  # Convert temperature from Celsius to Fahrenheit
  weather_df['temp_fahrenheit'] = weather_df['temp'] * 9 / 5 + 32

  weather_df = weather_df.drop(columns=['dwpt','rhum','prcp','snow','wpgt','pres','tsun', 'wspd', 'wdir', 'coco','dwpt','temp'])


  # Plot the Fahrenheit temperature
  weather_df.plot(y=['temp_fahrenheit'], title=f'Hourly Temperature in Houston (Fahrenheit) for {codeName}')
  plt.xlabel('Date and Time')
  plt.ylabel('Temperature (Â°F)')
  plt.show()
  return weather_df

# Create 'USAGE_START' as a combined datetime column before filtering for 'Alfa'
df['USAGE_START'] = pd.to_datetime(df['USAGE_DATE'].astype(str) + ' ' + df['USAGE_START_TIME'].astype(str), format='%m/%d/%Y %H:%M', errors='coerce')

# Handle NaT values (Not a Time) resulting from errors
df.dropna(subset=['USAGE_START'], inplace=True)
alfa_df = df[df['ESIID'] == 'Alfa']
bravo_df = df[df['ESIID'] == 'Bravo']
charlie_df = df[df['ESIID'] == 'Charlie']
delta_df = df[df['ESIID'] == 'Delta']
echo_df = df[df['ESIID'] == 'Echo']
foxtrot_df = df[df['ESIID'] == 'Foxtrot']
golf_df = df[df['ESIID'] == 'Golf']
hotel_df = df[df['ESIID'] == 'Hotel']
india_df = df[df['ESIID'] == 'India']
juliett_df = df[df['ESIID'] == 'Juliett']
kilo_df = df[df['ESIID'] == 'Kilo']
lima_df = df[df['ESIID'] == 'Lima']



# Set 'USAGE_START' as the index for time-based resampling
alfa_df.set_index('USAGE_START', inplace=True)
bravo_df.set_index('USAGE_START', inplace=True)
charlie_df.set_index('USAGE_START', inplace=True)
delta_df.set_index('USAGE_START', inplace=True)
echo_df.set_index('USAGE_START', inplace=True)
foxtrot_df.set_index('USAGE_START', inplace=True)
golf_df.set_index('USAGE_START', inplace=True)
hotel_df.set_index('USAGE_START', inplace=True)
india_df.set_index('USAGE_START', inplace=True)
juliett_df.set_index('USAGE_START', inplace=True)
kilo_df.set_index('USAGE_START', inplace=True)
lima_df.set_index('USAGE_START', inplace=True)

#fetches weather data for each user within that time frame
alfa_weather_df = fetch_weather_data("Alfa",2022, 10, 4, 2024, 11, 4)
bravo_weather_df = fetch_weather_data("Bravo", 2022, 10, 4, 2024, 11, 4)
charlie_weather_df = fetch_weather_data("Charlie",2022, 10, 4, 2024, 11, 1)
delta_weather_df = fetch_weather_data("Delta", 2022, 10, 4, 2024, 11, 1)
echo_weather_df = fetch_weather_data("Echo", 2022, 10, 4, 2024, 10, 30)
foxtrot_weather_df = fetch_weather_data("Foxtrox",2022, 10, 4, 2024, 11, 1)
golf_weather_df = fetch_weather_data("Golf",2022, 10, 4, 2024, 11, 1)
hotel_weather_df = fetch_weather_data("Hotel",2022, 10, 4, 2024, 10, 30)
india_weather_df = fetch_weather_data("India",2022, 10, 4, 2024, 11, 1)
juliett_weather_df = fetch_weather_data("Juliett",2022, 10, 4, 2024, 11, 1)
kilo_weather_df = fetch_weather_data("Kilo",2022, 10, 4, 2024, 11, 1)
lima_weather_df = fetch_weather_data("Lima",2022, 10, 4, 2024, 11, 1)
esiids = df['ESIID'].unique()

for esiid in esiids:
    # Filter data for the current ESIID
    esiid_df = df[df['ESIID'] == esiid]

    # Set 'USAGE_START' as the index for time-based plotting
    esiid_df.set_index('USAGE_START', inplace=True)

    # Plot the raw energy usage data for the current ESIID
    plt.figure(figsize=(30, 6))
    plt.plot(esiid_df.index, esiid_df['USAGE_KWH'], label=f'Energy Usage (KWH) for {esiid}', color='purple')
    plt.title(f'Raw Energy Usage for {esiid} Over Time')
    plt.xlabel('Time')
    plt.ylabel('Usage (KWH)')
    plt.legend()

    # Display the raw DataFrame for the current ESIID
    print(f"Raw DataFrame for {esiid}")
    print(esiid_df)

def merging_dataframes(user_df, temperature_df):
  print(f"{user_df.shape} Dataframe")
  # Set 'USAGE_START' as the index in bravo_df if not already
  user_df = user_df.reset_index()

  # Ensure both dataframes have datetime-compatible keys
  user_df['USAGE_START'] = pd.to_datetime(user_df['USAGE_START'])
  temperature_df['time'] = pd.to_datetime(temperature_df['time'])

  # Sort both dataframes by their datetime columns
  user_df = user_df.sort_values('USAGE_START')
  temperature_df = temperature_df.sort_values('time')

  # Use merge_asof to align temperature data to usage intervals
  merged_data_user = pd.merge_asof(
      user_df,
      temperature_df[['time', 'temp_fahrenheit']],  # Keep only time and temperature
      left_on='USAGE_START',  # Usage timestamp
      right_on='time',        # Closest hourly timestamp
      direction='backward'    # Match the closest previous hourly timestamp
  )

  # Drop unnecessary columns if needed
  merged_data_user = merged_data_user.drop(columns=['time'])

  # Print the resulting dataset
  print(merged_data_user)

  return merged_data_user

alfa_weather_df=alfa_weather_df.reset_index()
merged_data_alfa = merging_dataframes(alfa_df, alfa_weather_df)

bravo_weather_df = bravo_weather_df.reset_index()

merged_data_bravo = merging_dataframes(bravo_df, bravo_weather_df)

charlie_weather_df = charlie_weather_df.reset_index()
merged_data_charlie = merging_dataframes(charlie_df, charlie_weather_df)

delta_weather_df = delta_weather_df.reset_index()
merged_data_delta = merging_dataframes(delta_df, charlie_weather_df)

echo_weather_df=echo_weather_df.reset_index()
merged_data_echo = merging_dataframes(echo_df, echo_weather_df)

foxtrot_weather_df = foxtrot_weather_df.reset_index()
merged_data_foxtrot = merging_dataframes(foxtrot_df, foxtrot_weather_df)

golf_weather_df = golf_weather_df.reset_index()
merged_data_golf = merging_dataframes(golf_df, golf_weather_df)

hotel_weather_df = hotel_weather_df.reset_index()
merged_data_hotel = merging_dataframes(hotel_df, hotel_weather_df)

india_weather_df = india_weather_df.reset_index()
merged_data_india = merging_dataframes(india_df, india_weather_df)

juliett_weather_df = juliett_weather_df.reset_index()
merged_data_juliett = merging_dataframes(juliett_df, juliett_weather_df)

kilo_weather_df = kilo_weather_df.reset_index()
merged_data_kilo = merging_dataframes(kilo_df, kilo_weather_df)

lima_weather_df = lima_weather_df.reset_index()
merged_data_lima = merging_dataframes(lima_df, lima_weather_df)

from scipy.stats import pearsonr

corr, p_value = pearsonr(merged_data_alfa['USAGE_KWH'], merged_data_alfa['temp_fahrenheit'])
print(f"Correlation between Temperature and Usage_KWH: {corr}")
print(f"R^2: {corr**2}")

def create_many_to_many_sequences(data, input_sequence_length, output_sequence_length):
    X, y = [], []
    for i in range(len(data) - input_sequence_length - output_sequence_length + 1):
        # Input: first `input_sequence_length` timesteps
        X.append(data[i:i + input_sequence_length])
        # Output: next `output_sequence_length` timesteps
        y.append(data[i + input_sequence_length:i + input_sequence_length + output_sequence_length])
    return np.array(X), np.array(y)

def data_processing(dataset, merged_data, input_sequence_length, output_sequence_length):
  # Calculate the training data length (80%)
  training_len = math.ceil(len(dataset) * 0.8)  # Length for training

  # Ensure `merged_data` is a valid DataFrame
  assert isinstance(merged_data, pd.DataFrame), "merged_data_alfa should be a DataFrame"

  # Split the data into training and testing sets
  train_data = merged_data[:training_len]  # First 80% for training
  test_data = merged_data[training_len:]   # Remaining 20% for testing

  # Scale the 'USAGE_KWH' column
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaled_data = scaler.fit_transform(merged_data['USAGE_KWH'].values.reshape(-1, 1))

  # Split scaled data into training and testing sets
  train_scaled = scaled_data[:training_len]
  test_scaled = scaled_data[training_len:]

  # Create sequences
  X_train, y_train = create_many_to_many_sequences(train_scaled, input_sequence_length, output_sequence_length)

  # Create sequences for testing
  X_test, y_test = create_many_to_many_sequences(test_scaled, input_sequence_length, output_sequence_length)

  # Verify the shapes of the generated sequences
  print("X_train shape:", X_train.shape)  # Expected: (samples, input_sequence_length, features)
  print("y_train shape:", y_train.shape)  # Expected: (samples, output_sequence_length, features)
  print("X_test shape:", X_test.shape)
  print("y_test shape:", y_test.shape)
  return X_train, y_train, X_test, y_test, train_data, test_data, scaler, scaled_data

input_sequence_length = 60
output_sequence_length = 60
(x_train_alfa, y_train_alfa, x_test_alfa, y_test_alfa, train_data_alfa, test_data_alfa, scaler, alfa_scaled_data) = data_processing(alfa_df, merged_data_alfa, input_sequence_length, output_sequence_length)
(x_train_bravo, y_train_bravo, x_test_bravo, y_test_bravo, train_data_bravo, test_data_bravo, scaler, bravo_scaled_data) = data_processing(bravo_df, merged_data_bravo,input_sequence_length, output_sequence_length)
(x_train_charlie, y_train_charlie, x_test_charlie, y_test_charlie, train_data_charlie, test_data_charlie, scaler, charlie_scaled_data) = data_processing(charlie_df, merged_data_charlie,input_sequence_length, output_sequence_length)
(x_train_delta, y_train_delta, x_test_delta, y_test_delta, train_data_delta, test_data_delta, scaler, delta_scaled_data) = data_processing(delta_df, merged_data_delta,input_sequence_length, output_sequence_length)
(x_train_echo, y_train_echo, x_test_echo, y_test_echo, train_data_echo, test_data_echo, scaler, echo_scaled_data) = data_processing(echo_df, merged_data_echo,input_sequence_length, output_sequence_length)
(x_train_foxtrot, y_train_foxtrot, x_test_foxtrot, y_test_foxtrot, train_data_foxtrot, test_data_foxtrot, scaler, foxtrot_scaled_data) = data_processing(foxtrot_df, merged_data_foxtrot,input_sequence_length, output_sequence_length)
(x_train_golf, y_train_golf, x_test_golf, y_test_golf, train_data_golf, test_data_golf, scaler, golf_scaled_data) = data_processing(golf_df, merged_data_golf,input_sequence_length, output_sequence_length)
(x_train_hotel, y_train_hotel, x_test_hotel, y_test_hotel, train_data_hotel, test_data_hotel, scaler, hotel_scaled_data) = data_processing(hotel_df, merged_data_hotel,input_sequence_length, output_sequence_length)
(x_train_india, y_train_india, x_test_india, y_test_india, train_data_india, test_data_india, scaler, india_scaled_data) = data_processing(india_df, merged_data_india,input_sequence_length, output_sequence_length)
(x_train_juliett, y_train_juliett, x_test_juliett, y_test_juliett, train_data_juliett, test_data_juliett, scaler, juliett_scaled_data) = data_processing(juliett_df, merged_data_juliett,input_sequence_length, output_sequence_length)
(x_train_kilo, y_train_kilo, x_test_kilo, y_test_kilo, train_data_kilo, test_data_kilo, scaler, kilo_scaled_data) = data_processing(kilo_df, merged_data_kilo,input_sequence_length, output_sequence_length)
(x_train_lima, y_train_lima, x_test_lima, y_test_lima, train_data_lima, test_data_lima, scaler, lima_scaled_data) = data_processing(lima_df, merged_data_lima,input_sequence_length, output_sequence_length)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, TimeDistributed

#simple RNN model
def simple_RNN_model(name,X_train, y_train, X_test, y_test, input_sequence_length, output_sequence_length, scaler):
  simple_RNN = Sequential([
      Input(shape = (input_sequence_length, 1)),
      SimpleRNN(50, activation = 'relu', return_sequences = True),
      SimpleRNN(50, activation = 'relu', return_sequences = True),
      TimeDistributed(Dense(1))
  ])
  simple_RNN.summary()
  simple_RNN.compile(optimizer = 'adam', loss = 'mse')
  simple_RNN.fit(X_train, y_train, epochs = 3, batch_size = 128, validation_data = (X_test, y_test))
  loss = simple_RNN.evaluate(X_test, y_test)

  print(f"Test Loss: {loss}")
  predictions = simple_RNN.predict(X_test)

  #Simple RNN results
  simple_RNN_predictions_reshaped = predictions.reshape(-1, 1)  # Flatten to (samples * output_sequence_length, 1)
  y_test_reshaped = y_test.reshape(-1, 1)  # Flatten to (samples * output_sequence_length, 1)

  predicted_data_unscaled = scaler.inverse_transform(simple_RNN_predictions_reshaped)
  y_test_unscaled = scaler.inverse_transform(y_test.reshape(-1, 1))

  plt.figure(figsize=(30, 6))
  plt.plot(y_test_unscaled, label=f'Actual Energy Usage (KWH) for {name}', color='blue')
  plt.title(f'ActualEnergy Usage using Simple for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.show()

  plt.figure(figsize=(30, 6))
  plt.plot(predicted_data_unscaled, label=f'Predicted Energy Usage (KWH) for {name}', color='orange')
  plt.title(f'Predicted Energy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.show()

  return predicted_data_unscaled, simple_RNN

#Runs the simple RNN model
(alfa_predicted_data_unscaled, alfa_simple_RNN_model) = simple_RNN_model("Alfa", x_train_alfa, y_train_alfa, x_test_alfa, y_test_alfa, input_sequence_length, output_sequence_length, scaler)
#(bravo_predicted_data_unscaled, bravo_simple_RNN_model) = simple_RNN_model("Bravo",x_train_bravo, y_train_bravo, x_test_bravo, y_test_bravo, input_sequence_length, output_sequence_length, scaler)
#(charlie_predicted_data_unscaled, charlie_simple_RNN_model) = simple_RNN_model("Charlie",x_train_charlie, y_train_charlie, x_test_charlie, y_test_charlie, input_sequence_length, output_sequence_length, scaler)
(delta_predicted_data_unscaled, delta_simple_RNN_model) = simple_RNN_model("Delta",x_train_delta, y_train_delta, x_test_delta, y_test_delta, input_sequence_length, output_sequence_length, scaler)
#(echo_predicted_data_unscaled, echo_simple_RNN_model) = simple_RNN_model("Echo",x_train_echo, y_train_echo, x_test_echo, y_test_echo, input_sequence_length, output_sequence_length, scaler)
#(foxtrot_predicted_data_unscaled, foxtrot_simple_RNN_model) = simple_RNN_model("Foxtrot",x_train_foxtrot, y_train_foxtrot, x_test_foxtrot, y_test_foxtrot,input_sequence_length, output_sequence_length, scaler)
#(golf_predicted_data_unscaled, golf_simple_RNN_model) = simple_RNN_model("Golf",x_train_golf, y_train_golf, x_test_golf, y_test_golf,input_sequence_length, output_sequence_length, scaler)
#(hotel_predicted_data_unscaled, hotel_simple_RNN_model) = simple_RNN_model("Hotel",x_train_hotel, y_train_hotel, x_test_hotel, y_test_hotel,input_sequence_length, output_sequence_length, scaler)
#(india_predicted_data_unscaled, india_simple_RNN_model) = simple_RNN_model("India",x_train_india, y_train_india, x_test_india, y_test_india,input_sequence_length, output_sequence_length, scaler)
#(juliett_predicted_data_unscaled, juliett_simple_RNN_model) = simple_RNN_model("Juliett",x_train_juliett, y_train_juliett, x_test_juliett, y_test_juliett,input_sequence_length, output_sequence_length, scaler)
#(kilo_predicted_data_unscaled, kilo_simple_RNN_model) = simple_RNN_model("Kilo",x_train_kilo, y_train_kilo, x_test_kilo, y_test_kilo,input_sequence_length, output_sequence_length, scaler)
(lima_predicted_data_unscaled, lima_simple_RNN_model) = simple_RNN_model("Lima",x_train_lima, y_train_lima, x_test_lima, y_test_lima,input_sequence_length, output_sequence_length, scaler)

def LSTM_model(name,X_train, y_train, x_test, y_test):
  # Define the many-to-many LSTM model
  model = Sequential()
  model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)))
  model.add(LSTM(50, activation='relu', return_sequences=True))
  model.add(TimeDistributed(Dense(1)))  # Output one value for each timestep in the output sequence

  # Compile the model
  model.compile(optimizer='adam', loss='mse')

  # Print model summary
  model.summary()

  model.fit(X_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=128) #changed epoch for time sake
  loss = model.evaluate(x_test, y_test, verbose=0)
  print(f"Test Loss: {loss}")

  # Predict on the test set
  predicted_data = model.predict(x_test)  # Shape: (samples, output_sequence_length, features)

  # Reshape for inverse transformation
  predicted_data_reshaped = predicted_data.reshape(-1, 1)  # Flatten to (samples * output_sequence_length, 1)
  y_test_reshaped = y_test.reshape(-1, 1)

  # Inverse transform predictions and actual values
  predicted_data_unscaled = scaler.inverse_transform(predicted_data_reshaped)
  y_test_unscaled = scaler.inverse_transform(y_test_reshaped)

  # Plot actual vs. predicted values
  plt.figure(figsize=(30, 6))
  plt.plot(y_test_unscaled, label=f'Actual Energy Usage (KWH) for {name}', color='blue', alpha=0.7)
  plt.plot(predicted_data_unscaled, label=f'Predicted Energy Usage (KWH) for{name}', color='orange', alpha=0.7)
  plt.title(f'Actual vs Predicted Energy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.grid(True)
  plt.show()

  # Plot actual values separately
  plt.figure(figsize=(30, 6))
  plt.plot(y_test_unscaled, label=f'Actual Energy Usage (KWH) for {name}', color='blue')
  plt.title(f'Actual Energy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.grid(True)
  plt.show()

  # Plot predicted values separately
  plt.figure(figsize=(30, 6))
  plt.plot(predicted_data_unscaled, label=f'Predicted Energy Usage (KWH) for {name}', color='orange')
  plt.title(f'Predicted Energy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.grid(True)
  plt.show()

  return model, predicted_data_unscaled

#runs and measures the performance of the LSTM model
(alfa_LSTM_model, alfa_LSTM_predicted_data_unscaled)= LSTM_model("Alfa",x_train_alfa, y_train_alfa, x_test_alfa, y_test_alfa)
#(bravo_LSTM_model, bravo_LSTM_predicted_data_unscaled)= LSTM_model("Bravo",x_train_bravo, y_train_bravo, x_test_bravo, y_test_bravo)
#(charlie_LSTM_model, charlie_LSTM_predicted_data_unscaled)= LSTM_model("Charlie",x_train_charlie, y_train_charlie, x_test_charlie, y_test_charlie)
(delta_LSTM_model, delta_LSTM_predicted_data_unscaled)= LSTM_model("Delta",x_train_delta, y_train_delta, x_test_delta, y_test_delta)
#(echo_LSTM_model, echo_LSTM_predicted_data_unscaled)= LSTM_model("Echo",x_train_echo, y_train_echo, x_test_echo, y_test_echo)
#(foxtrot_LSTM_model, foxtrot_LSTM_predicted_data_unscaled)= LSTM_model("Foxtrot",x_train_foxtrot, y_train_foxtrot, x_test_foxtrot, y_test_foxtrot)
#(golf_LSTM_model, golf_LSTM_predicted_data_unscaled)= LSTM_model("Golf",x_train_golf, y_train_golf, x_test_golf, y_test_golf)
#(hotel_LSTM_model, hotel_LSTM_predicted_data_unscaled)= LSTM_model("hotel",x_train_hotel, y_train_hotel, x_test_hotel, y_test_hotel)
#(india_LSTM_model, india_LSTM_predicted_data_unscaled)= LSTM_model("India",x_train_india, y_train_india, x_test_india, y_test_india)
#(juliett_LSTM_model, juliett_LSTM_predicted_data_unscaled)= LSTM_model("Juliett",x_train_juliett, y_train_juliett, x_test_juliett, y_test_juliett)
#(kilo_LSTM_model, kilo_LSTM_predicted_data_unscaled)= LSTM_model("Kilo",x_train_kilo, y_train_kilo, x_test_kilo, y_test_kilo)
(lima_LSTM_model, lima_LSTM_predicted_data_unscaled)= LSTM_model("Lima",x_train_lima, y_train_lima, x_test_lima, y_test_lima)

def GRU_Model(name, x_train, y_train, x_test, y_test):
  # GRU Model
  regressorGRU = Sequential()

  # GRU layers with Dropout regularisation
  regressorGRU.add(GRU(units=50,
                      return_sequences=True,
                      input_shape=(x_train.shape[1],1),
                      activation='tanh'))
  regressorGRU.add(Dropout(0.2))

  regressorGRU.add(GRU(units=50,
                      return_sequences=True,
                      activation='tanh'))

  # The output layer
  regressorGRU.add(TimeDistributed(Dense(units=1, activation='relu')))
  # Compiling the RNN
  regressorGRU.compile(optimizer=SGD(learning_rate=0.01,
                                    decay=1e-7,
                                    momentum=0.9,
                                    nesterov=False),
                      loss='mean_squared_error')

  # Fitting the data
  regressorGRU.fit(x_train, y_train, epochs=3, batch_size=128,validation_data = (x_test, y_test))
  loss = regressorGRU.evaluate(x_test, y_test, verbose=0)
  print(f"Test Loss: {loss}")
  regressorGRU.summary()

  #Test Predictions and plotting for RNN GRU
  prediction_GRU = regressorGRU.predict(x_test)

  prediction_GRU_reshaped = prediction_GRU.reshape(-1, prediction_GRU.shape[-1]) # Reshape to (samples * output_sequence_length, features)

  y_test_reshaped = y_test.reshape(-1, 1)

  predicted_data_unscaled2 = scaler.inverse_transform(prediction_GRU_reshaped)
  y_test_unscaled2 = scaler.inverse_transform(y_test.reshape(-1, 1))

  plt.figure(figsize=(30, 6))
  plt.plot(y_test_unscaled2, label=f'Actual Energy Usage (KWH) for {name}', color='blue')
  plt.title(f'ActualEnergy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.show()

  plt.figure(figsize=(30, 6))
  plt.plot(predicted_data_unscaled2, label=f'Predicted Energy Usage (KWH) for {name}', color='orange')
  plt.title(f'Predicted Energy Usage for {name}')
  plt.xlabel('Time')
  plt.ylabel('Energy Usage (KWH)')
  plt.legend()
  plt.show()
  return regressorGRU,predicted_data_unscaled2

#add in other users
(alfa_GRU_model, alfa_GRU_predicted_data_unscaled) = GRU_Model("Alfa",x_train_alfa, y_train_alfa, x_test_alfa, y_test_alfa)
#(bravo_GRU_model, bravo_GRU_predicted_data_unscaled) = GRU_Model("Bravo",x_train_bravo, y_train_bravo, x_test_bravo, y_test_bravo)
#(charlie_GRU_model, charlie_GRU_predicted_data_unscaled) = GRU_Model("Charlie",x_train_charlie, y_train_charlie, x_test_charlie, y_test_charlie)
(delta_GRU_model, delta_GRU_predicted_data_unscaled) = GRU_Model("Delta",x_train_delta, y_train_delta, x_test_delta, y_test_delta)
#(echo_GRU_model, echo_GRU_predicted_data_unscaled) = GRU_Model("Echo",x_train_echo, y_train_echo, x_test_echo, y_test_echo)
#(foxtrot_GRU_model, foxtrot_GRU_predicted_data_unscaled) = GRU_Model("Foxtrot",x_train_foxtrot, y_train_foxtrot, x_test_foxtrot, y_test_foxtrot)
#(golf_GRU_model, golf_GRU_predicted_data_unscaled) = GRU_Model("Golf",x_train_golf, y_train_golf, x_test_golf, y_test_golf)
#(hotel_GRU_model, hotel_GRU_predicted_data_unscaled) = GRU_Model("Hotel",x_train_hotel, y_train_hotel, x_test_hotel, y_test_hotel)
#(india_GRU_model, india_GRU_predicted_data_unscaled) = GRU_Model("India",x_train_india, y_train_india, x_test_india, y_test_india)
#(juliett_GRU_model, juliett_GRU_predicted_data_unscaled) = GRU_Model("Juliett",x_train_juliett, y_train_juliett, x_test_juliett, y_test_juliett)
#(kilo_GRU_model, kilo_GRU_predicted_data_unscaled) = GRU_Model("Kilo",x_train_kilo, y_train_kilo, x_test_kilo, y_test_kilo)
(lima_GRU_model, lima_GRU_predicted_data_unscaled) = GRU_Model("Lima",x_train_lima, y_train_lima, x_test_lima, y_test_lima)

def plot_results(name,train_data, test_data, predicted_data_unscaled):
  # Plot the training, test, and predicted data together
  plt.figure(figsize=(45, 12))

  # Training data
  plt.plot(train_data.index, train_data['USAGE_KWH'], label='Training Data', color='blue')

  # Test data
  plt.plot(test_data.index, test_data['USAGE_KWH'], label='Test Data', color='green')

  # Predicted data
  predicted_data_unscaled_adjusted = predicted_data_unscaled[:len(test_data) - input_sequence_length]
  test_time_index = test_data.index[input_sequence_length:len(predicted_data_unscaled) + input_sequence_length]
  plt.plot(test_time_index, predicted_data_unscaled_adjusted, label=f'Predicted Data for {name}(LSTM)', color='orange')

  # Adding title and labels
  plt.title(f'Energy Usage Prediction for {name}(Train vs Test vs Predicted)', fontsize=18)
  plt.xlabel('Time', fontsize=14)
  plt.ylabel('Energy Usage (KWH)', fontsize=14)
  plt.legend(fontsize=12)
  plt.grid(True)
  plt.show()

print("Plot for Simple RNN Energy Usage Prediction (Train vs Test vs Predicted)")
plot_results("Alfa",train_data_alfa, test_data_alfa, alfa_predicted_data_unscaled)
#plot_results("Bravo",train_data_bravo, test_data_bravo, bravo_predicted_data_unscaled)
#plot_results("Charlie",train_data_charlie, test_data_charlie, charlie_predicted_data_unscaled)
plot_results("Delta",train_data_delta, test_data_delta, delta_predicted_data_unscaled)
#plot_results("Echo",train_data_echo, test_data_echo, echo_predicted_data_unscaled)
#plot_results("Foxtrot",train_data_foxtrot, test_data_foxtrot, foxtrot_predicted_data_unscaled)
#plot_results("Golf",train_data_golf, test_data_golf, golf_predicted_data_unscaled)
#plot_results("Hotel",train_data_hotel, test_data_hotel, hotel_predicted_data_unscaled)
#plot_results("India",train_data_india, test_data_india, india_predicted_data_unscaled)
#plot_results("Juliett",train_data_juliett, test_data_juliett, juliett_predicted_data_unscaled)
#plot_results("Kilo",train_data_kilo, test_data_kilo, kilo_predicted_data_unscaled)
plot_results("Lima",train_data_lima, test_data_lima, lima_predicted_data_unscaled)

#plot for LSTM
# Plot the training, test, and predicted for GRU Plot together
def LSTM_plot_results(name, train_data, test_data, predicted_data_unscaled):
  plt.figure(figsize=(45, 12))

  # Training data
  plt.plot(train_data.index, train_data['USAGE_KWH'], label='Training Data', color='blue')

  # Test data
  plt.plot(test_data.index, test_data['USAGE_KWH'], label='Test Data', color='green')

  # Predicted data
  # Calculate the correct length for reshaping
  reshape_length = (len(test_data) - input_sequence_length)
  # Reshape predicted_data_unscaled to match the number of test data points
  # We are taking only the first element of each prediction sequence
  predicted_data_unscaled_reshaped = predicted_data_unscaled[:reshape_length, 0]
  #The assumption being that the total prediction output shape is just a multiple of your desired test size
  test_time_index = test_data.index[input_sequence_length:len(test_data)] #Adjust to index to the right size
  plt.plot(test_time_index, predicted_data_unscaled_reshaped, label=f'Predicted Data for {name}(GRU)', color='orange')

  # Adding title and labels
  plt.title(f'Energy Usage Prediction for {name}(Train vs Test vs Predicted)', fontsize=18)
  plt.xlabel('Time', fontsize=14)
  plt.ylabel('Energy Usage (KWH)', fontsize=14)
  plt.legend(fontsize=12)
  plt.grid(True)
  plt.show()

LSTM_plot_results("Alfa",train_data_alfa, test_data_alfa, alfa_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Bravo",train_data_bravo, test_data_bravo, bravo_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Charlie",train_data_charlie, test_data_charlie, charlie_LSTM_predicted_data_unscaled)
LSTM_plot_results("Delta",train_data_delta, test_data_delta, delta_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Echo",train_data_echo, test_data_echo, echo_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Foxtrot",train_data_foxtrot, test_data_foxtrot, foxtrot_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Golf",train_data_golf, test_data_golf, golf_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Hotel",train_data_hotel, test_data_hotel, hotel_LSTM_predicted_data_unscaled)
#LSTM_plot_results("India",train_data_india, test_data_india, india_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Juliett",train_data_juliett, test_data_juliett, juliett_LSTM_predicted_data_unscaled)
#LSTM_plot_results("Kilo",train_data_kilo, test_data_kilo, kilo_LSTM_predicted_data_unscaled)
LSTM_plot_results("Lima",train_data_lima, test_data_lima, lima_LSTM_predicted_data_unscaled)

# Plot the training, test, and predicted for GRU Plot together
def GRU_plot_results(name, train_data, test_data, predicted_data_unscaled):
  plt.figure(figsize=(45, 12))

  # Training data
  plt.plot(train_data.index, train_data['USAGE_KWH'], label='Training Data', color='blue')

  # Test data
  plt.plot(test_data.index, test_data['USAGE_KWH'], label='Test Data', color='green')

  # Predicted data
  # Calculate the correct length for reshaping
  reshape_length = (len(test_data) - input_sequence_length)
  # Reshape predicted_data_unscaled to match the number of test data points
  # We are taking only the first element of each prediction sequence
  predicted_data_unscaled_reshaped = predicted_data_unscaled[:reshape_length, 0]
  #The assumption being that the total prediction output shape is just a multiple of your desired test size
  test_time_index = test_data.index[input_sequence_length:len(test_data)] #Adjust to index to the right size
  plt.plot(test_time_index, predicted_data_unscaled_reshaped, label=f'Predicted Data (GRU) for {name}', color='orange')

  # Adding title and labels
  plt.title(f'Energy Usage Prediction for {name}(Train vs Test vs Predicted)', fontsize=18)
  plt.xlabel('Time', fontsize=14)
  plt.ylabel('Energy Usage (KWH)', fontsize=14)
  plt.legend(fontsize=12)
  plt.grid(True)
  plt.show()

#add in for other users
GRU_plot_results("Alfa",train_data_alfa, test_data_alfa, alfa_GRU_predicted_data_unscaled)
#GRU_plot_results("Bravo",train_data_bravo, test_data_bravo, bravo_GRU_predicted_data_unscaled)
#GRU_plot_results("Charlie",train_data_charlie, test_data_charlie, charlie_GRU_predicted_data_unscaled)
GRU_plot_results("Delta",train_data_delta, test_data_delta, delta_GRU_predicted_data_unscaled)
#GRU_plot_results("Echo",train_data_echo, test_data_echo, echo_GRU_predicted_data_unscaled)
#GRU_plot_results("Foxtrot",train_data_foxtrot, test_data_foxtrot, foxtrot_GRU_predicted_data_unscaled)
#GRU_plot_results("Golf",train_data_golf, test_data_golf, golf_GRU_predicted_data_unscaled)
#GRU_plot_results("Hotel",train_data_hotel, test_data_hotel, hotel_GRU_predicted_data_unscaled)
#GRU_plot_results("India",train_data_india, test_data_india, india_GRU_predicted_data_unscaled)
#GRU_plot_results("Juliett",train_data_juliett, test_data_juliett, juliett_GRU_predicted_data_unscaled)
#GRU_plot_results("Kilo",train_data_kilo, test_data_kilo, kilo_GRU_predicted_data_unscaled)
GRU_plot_results("Lima",train_data_lima, test_data_lima, lima_GRU_predicted_data_unscaled)

print("\nFORECASTING DATA\n")

# Forecast iteratively for extended periods
def forecasting_data(model, scaled_data, input_sequence_length):

  # Use the last sequence from the dataset for prediction
  last_sequence = scaled_data[-input_sequence_length:]
  input_sequence = np.reshape(last_sequence, (1, input_sequence_length, 1))  # Reshape for the model


  # Predict the next sequence
  forecasted_sequence = model.predict(input_sequence)

  # Inverse scale the forecasted sequence
  forecasted_sequence_unscaled = scaler.inverse_transform(forecasted_sequence.reshape(-1, 1))
  forecast_steps = 2882  # 1 month = 2880 timesteps
  noise_level = 0.05
  last_sequence = scaled_data[-input_sequence_length:]  # Last input sequence for forecasting
  input_sequence = last_sequence.copy()  # Keep a separate copy of the input sequence
  forecasted_data = []

  # Forecast iteratively for 1 month (or any defined period)
  while len(forecasted_data) < forecast_steps:
      # Predict the next sequence using the model
      next_sequence = model.predict(input_sequence.reshape(1, input_sequence_length, 1), verbose=0)[0]
      forecasted_data.extend(next_sequence.flatten())  # Append the predictions to the forecasted data

      # Update the input sequence for the next prediction
      input_sequence = np.append(input_sequence, next_sequence, axis=0)[-input_sequence_length:]  # Rolling window

  # Trim forecasted data to the required number of steps
  forecasted_data = forecasted_data[:forecast_steps]

  # Add Gaussian noise directly to forecasted data
  forecasted_data = np.array(forecasted_data) + np.random.normal(
        loc=0, scale=noise_level, size=len(forecasted_data)
    )

  # Inverse scale the forecasted data
  forecasted_data_unscaled = scaler.inverse_transform(np.array(forecasted_data).reshape(-1, 1))

  # Generate future timestamps for 15-minute intervals
  last_timestamp = merged_data_alfa['USAGE_START'].iloc[-1]  # Assuming 'USAGE_START' exists in `merged_data_alfa`
  future_timestamps = pd.date_range(start=last_timestamp, periods=forecast_steps, freq='15min')

  # Create a DataFrame for the forecast
  forecast_df = pd.DataFrame(data=forecasted_data_unscaled, index=future_timestamps, columns=['Forecasted Usage'])

  # Plot historical and forecasted data
  plt.figure(figsize=(20, 10))

  # Plot historical data
  plt.plot(merged_data_alfa['USAGE_START'], merged_data_alfa['USAGE_KWH'], label='Historical Data', color='blue')

  # Plot forecasted data
  plt.plot(forecast_df.index, forecast_df['Forecasted Usage'], label='Forecasted Data', color='red')

  # Add confidence intervals (optional)
  upper_bound = forecasted_data_unscaled + (1.96 * np.std(forecasted_data_unscaled))  # 95% CI
  lower_bound = forecasted_data_unscaled - (1.96 * np.std(forecasted_data_unscaled))
  plt.fill_between(
      forecast_df.index,
      lower_bound.flatten(),
      upper_bound.flatten(),
      color='yellow',
      alpha=0.3,
      label='95% Confidence Interval'
  )

  # Add title and labels
  plt.title('Energy Usage Forecast for Next Month', fontsize=18)
  plt.xlabel('Time', fontsize=14)
  plt.ylabel('Energy Usage (KWH)', fontsize=14)
  plt.legend(fontsize=12)
  plt.grid(True)
  plt.show()

  # Print a sample of the forecasted DataFrame
  print(forecast_df)
  return forecast_df

#gets the forcasting results for Simple RNN
alfa_forecast_simpleRNN = forecasting_data(alfa_simple_RNN_model, alfa_scaled_data, input_sequence_length)
#bravo_forecast_simpleRNN = forecasting_data(bravo_simple_RNN_model, bravo_scaled_data, input_sequence_length)
#charlie_forecast_simpleRNN = forecasting_data(charlie_simple_RNN_model, charlie_scaled_data, input_sequence_length)
delta_forecast_simpleRNN = forecasting_data(delta_simple_RNN_model, delta_scaled_data, input_sequence_length)
#echo_forecast_simpleRNN = forecasting_data(echo_simple_RNN_model, echo_scaled_data, input_sequence_length)
#foxtrot_forecast_simpleRNN = forecasting_data(foxtrot_simple_RNN_model, foxtrot_scaled_data, input_sequence_length)
#golf_forecast_simpleRNN = forecasting_data(golf_simple_RNN_model, golf_scaled_data, input_sequence_length)
#hotel_forecast_simpleRNN = forecasting_data(hotel_simple_RNN_model, hotel_scaled_data, input_sequence_length)
#india_forecast_simpleRNN = forecasting_data(india_simple_RNN_model, india_scaled_data, input_sequence_length)
#juliett_forecast_simpleRNN = forecasting_data(juliett_simple_RNN_model, juliett_scaled_data, input_sequence_length)
#kilo_forecast_simpleRNN = forecasting_data(kilo_simple_RNN_model, kilo_scaled_data, input_sequence_length)
lima_forecast_simpleRNN = forecasting_data(lima_simple_RNN_model, lima_scaled_data, input_sequence_length)

#forecasting results for LSTM
alfa_forecast_df_LSTM = forecasting_data(alfa_LSTM_model, alfa_scaled_data, input_sequence_length)
#bravo_forecast_df_LSTM = forecasting_data(bravo_LSTM_model, bravo_scaled_data, input_sequence_length)
#charlie_forecast_df_LSTM = forecasting_data(charlie_LSTM_model, charlie_scaled_data, input_sequence_length)
delta_forecast_df_LSTM = forecasting_data(delta_LSTM_model, delta_scaled_data, input_sequence_length)
#echo_forecast_df_LSTM = forecasting_data(echo_LSTM_model, echo_scaled_data, input_sequence_length)
#foxtrot_forecast_df_LSTM = forecasting_data(foxtrot_LSTM_model, foxtrot_scaled_data, input_sequence_length)
#golf_forecast_df_LSTM = forecasting_data(golf_LSTM_model, golf_scaled_data, input_sequence_length)
#hotel_forecast_df_LSTM = forecasting_data(hotel_LSTM_model, hotel_scaled_data, input_sequence_length)
#india_forecast_df_LSTM = forecasting_data(india_LSTM_model, india_scaled_data, input_sequence_length)
#juliett_forecast_df_LSTM = forecasting_data(juliett_LSTM_model, juliett_scaled_data, input_sequence_length)
#kilo_forecast_df_LSTM = forecasting_data(kilo_LSTM_model, kilo_scaled_data, input_sequence_length)
lima_forecast_df_LSTM = forecasting_data(lima_LSTM_model, lima_scaled_data, input_sequence_length)

alfa_forecast_df_GRU = forecasting_data(alfa_GRU_model, alfa_scaled_data, input_sequence_length)
#bravo_forecast_df_GRU = forecasting_data(bravo_GRU_model, bravo_scaled_data, input_sequence_length)
#charlie_forecast_df_GRU = forecasting_data(charlie_GRU_model, charlie_scaled_data, input_sequence_length)
delta_forecast_df_GRU = forecasting_data(delta_GRU_model, delta_scaled_data, input_sequence_length)
#echo_forecast_df_GRU = forecasting_data(echo_GRU_model, echo_scaled_data, input_sequence_length)
#foxtrot_forecast_df_GRU = forecasting_data(foxtrot_GRU_model, foxtrot_scaled_data, input_sequence_length)
#golf_forecast_df_GRU = forecasting_data(golf_GRU_model, golf_scaled_data, input_sequence_length)
#hotel_forecast_df_GRU = forecasting_data(hotel_GRU_model, hotel_scaled_data, input_sequence_length)
#india_forecast_df_GRU = forecasting_data(india_GRU_model, india_scaled_data, input_sequence_length)
#juliett_forecast_df_GRU = forecasting_data(juliett_GRU_model, juliett_scaled_data, input_sequence_length)
#kilo_forecast_df_GRU = forecasting_data(kilo_GRU_model, kilo_scaled_data, input_sequence_length)
lima_forecast_df_GRU = forecasting_data(lima_GRU_model, lima_scaled_data, input_sequence_length)

def forecast_plot(name, forecast_df, merged_data, year, month):
  # Convert 'USAGE_START' to datetime before using
    merged_data['USAGE_START'] = pd.to_datetime(merged_data['USAGE_START'])

    # Forecasted usage data
    plt.figure(figsize=(40, 12))
    plt.plot(forecast_df.index, forecast_df['Forecasted Usage'], label=f'Forecasted Data for {name}', color='red')
    plt.title(f'Forecasted Usage for {name}')
    plt.xlabel('Time')
    plt.ylabel('Energy Usage (KWH)')
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.show()

    # Filter data for their Start time
    month_forecast = merged_data[
        (merged_data['USAGE_START'].dt.year == year) &
        (merged_data['USAGE_START'].dt.month == month)
    ]

    # Temperature plot
    plt.figure(figsize=(40, 12))
    plt.plot(month_forecast['USAGE_START'], month_forecast['temp_fahrenheit'], label=f'Temperature for {name}', color='blue')
    plt.title(f'Temperature for {name}')
    plt.xlabel('Time')
    plt.ylabel('Temperature (F)')
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.show()

#plot forecast for Simple RNN
forecast_plot("Alfa", alfa_forecast_simpleRNN, merged_data_alfa, 2023, 10)
#forecast_plot("Bravo", bravo_forecast_simpleRNN, merged_data_bravo, 2023, 11)
#forecast_plot("Charlie", charlie_forecast_simpleRNN, merged_data_charlie, 2023, 11)
forecast_plot("Delta", delta_forecast_simpleRNN, merged_data_delta, 2023, 11)
#forecast_plot("Echo", echo_forecast_simpleRNN, merged_data_echo, 2023, 10)
#forecast_plot("Foxtrot", foxtrot_forecast_simpleRNN, merged_data_foxtrot, 2023, 11)
#forecast_plot("Golf", golf_forecast_simpleRNN, merged_data_golf, 2023, 11)
#forecast_plot("Hotel", hotel_forecast_simpleRNN, merged_data_hotel, 2023, 10)
#forecast_plot("India", india_forecast_simpleRNN, merged_data_india, 2023, 11)
#forecast_plot("Juliett", juliett_forecast_simpleRNN, merged_data_juliett, 2023, 11)
#forecast_plot("Kilo", kilo_forecast_simpleRNN, merged_data_kilo, 2023, 11)
forecast_plot("Lima", lima_forecast_simpleRNN, merged_data_lima, 2023, 10)

#plot forecasting for LSTM
forecast_plot("Alfa", alfa_forecast_df_LSTM, merged_data_alfa, 2023, 10)
#forecast_plot("Bravo", bravo_forecast_df_LSTM, merged_data_bravo, 2023, 11)
#forecast_plot("Charlie", charlie_forecast_df_LSTM, merged_data_charlie, 2023, 11)
forecast_plot("Delta", delta_forecast_df_LSTM, merged_data_delta, 2023, 11)
#forecast_plot("Echo", echo_forecast_df_LSTM, merged_data_echo, 2023, 10)
#forecast_plot("Foxtrot", foxtrot_forecast_df_LSTM, merged_data_foxtrot, 2023, 11)
#forecast_plot("Golf", golf_forecast_df_LSTM, merged_data_golf, 2023, 11)
#forecast_plot("Hotel", hotel_forecast_df_LSTM, merged_data_hotel, 2023, 10)
#forecast_plot("India", india_forecast_df_LSTM, merged_data_india, 2023, 11)
#forecast_plot("Juliett", juliett_forecast_df_LSTM, merged_data_juliett, 2023, 11)
#forecast_plot("Kilo", kilo_forecast_df_LSTM, merged_data_kilo, 2023, 11)
forecast_plot("Lima", lima_forecast_df_LSTM, merged_data_lima, 2023, 11)

#plot forecasting for GRU
forecast_plot("Alfa", alfa_forecast_df_GRU, merged_data_alfa, 2023, 10)
#forecast_plot("Bravo", bravo_forecast_df_GRU, merged_data_bravo, 2023, 11)
#forecast_plot("Charlie", charlie_forecast_df_GRU, merged_data_charlie, 2023, 11)
forecast_plot("Delta", delta_forecast_df_GRU, merged_data_delta, 2023, 11)
#forecast_plot("Echo", echo_forecast_df_GRU, merged_data_echo, 2023, 10)
#forecast_plot("Foxtrot", foxtrot_forecast_df_GRU, merged_data_foxtrot, 2023, 11)
#forecast_plot("Golf", golf_forecast_df_GRU, merged_data_golf, 2023, 11)
#forecast_plot("Hotel", hotel_forecast_df_GRU, merged_data_hotel, 2023, 10)
#forecast_plot("India", india_forecast_df_GRU, merged_data_india, 2023, 11)
#forecast_plot("Juliett", juliett_forecast_df_GRU, merged_data_juliett, 2023, 11)
#forecast_plot("Kilo", kilo_forecast_df_GRU, merged_data_kilo, 2023, 11)
forecast_plot("Lima", lima_forecast_df_GRU, merged_data_lima, 2023, 11)

from scipy.stats import pearsonr

# Alfa
corr_alfa, p_value_alfa = pearsonr(
    merged_data_alfa['USAGE_KWH'],
    merged_data_alfa['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Alfa): {corr_alfa}")
print(f"R^2 (Alfa): {corr_alfa**2}")

# Bravo
corr_bravo, p_value_bravo = pearsonr(
    merged_data_bravo['USAGE_KWH'],
    merged_data_bravo['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Bravo): {corr_bravo}")
print(f"R^2 (Bravo): {corr_bravo**2}")

# Charlie
corr_charlie, p_value_charlie = pearsonr(
    merged_data_charlie['USAGE_KWH'],
    merged_data_charlie['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Charlie): {corr_charlie}")
print(f"R^2 (Charlie): {corr_charlie**2}")

# Delta
corr_delta, p_value_delta = pearsonr(
    merged_data_delta['USAGE_KWH'],
    merged_data_delta['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Delta): {corr_delta}")
print(f"R^2 (Delta): {corr_delta**2}")

# Echo
corr_echo, p_value_echo = pearsonr(
    merged_data_echo['USAGE_KWH'],
    merged_data_echo['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Echo): {corr_echo}")
print(f"R^2 (Echo): {corr_echo**2}")

# Foxtrot
corr_foxtrot, p_value_foxtrot = pearsonr(
    merged_data_foxtrot['USAGE_KWH'],
    merged_data_foxtrot['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Foxtrot): {corr_foxtrot}")
print(f"R^2 (Foxtrot): {corr_foxtrot**2}")

# Golf
corr_golf, p_value_golf = pearsonr(
    merged_data_golf['USAGE_KWH'],
    merged_data_golf['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Golf): {corr_golf}")
print(f"R^2 (Golf): {corr_golf**2}")

# Hotel
corr_hotel, p_value_hotel = pearsonr(
    merged_data_hotel['USAGE_KWH'],
    merged_data_hotel['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Hotel): {corr_hotel}")
print(f"R^2 (Hotel): {corr_hotel**2}")

# India
corr_india, p_value_india = pearsonr(
    merged_data_india['USAGE_KWH'],
    merged_data_india['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (India): {corr_india}")
print(f"R^2 (India): {corr_india**2}")
'''
# Juliet
corr_juliet, p_value_juliet = pearsonr(
    merged_data_juliet['USAGE_KWH'],
    merged_data_juliet['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Juliet): {corr_juliet}")
print(f"R^2 (Juliet): {corr_juliet**2}")
'''
# Kilo
corr_kilo, p_value_kilo = pearsonr(
    merged_data_kilo['USAGE_KWH'],
    merged_data_kilo['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Kilo): {corr_kilo}")
print(f"R^2 (Kilo): {corr_kilo**2}")

# Lima
corr_lima, p_value_lima = pearsonr(
    merged_data_lima['USAGE_KWH'],
    merged_data_lima['temp_fahrenheit']
)
print(f"Correlation between Temperature and Usage_KWH (Lima): {corr_lima}")
print(f"R^2 (Lima): {corr_lima**2}")